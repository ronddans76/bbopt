"""
The hyperopt backend. Does black box optimization using hyperopt.
"""

# Imports:

import numpy as np

from hyperopt import (
    hp,
    tpe,
    FMinIter,
)
from hyperopt.pyll import as_apply
from hyperopt.base import (
    Domain,
    Trials,
    STATUS_OK,
    STATUS_RUNNING,
    JOB_STATE_DONE,
    spec_from_misc,
)

from bbopt.backends.random import RandomBackend
from bbopt.params import param_processor
from bbopt.util import (
    sorted_items,
    negate_objective,
    format_err,
    make_features,
    serve_values,
)

# Utilities:

def create_space(
    name,
    choice=None,
    randrange=None,
    uniform=None,
    normalvariate=None,
):
    """Create a hyperopt space for the given param kwargs."""
    if choice is not None:
        return hp.choice(name, *choice)
    if randrange is not None:
        start, stop, step = randrange
        if step != 1:
            raise ValueError("the hyperopt backend only supports a randrange step size of 1")
        return start + hp.randint(name, stop)  # despite being called randint, stop is exclusive
    if uniform is not None:
        return hp.uniform(name, *uniform)
    if normalvariate is not None:
        return hp.normal(name, *normalvariate)
    raise TypeError("insufficiently specified parameter %r" % name)

def examples_to_trials(examples, params):
    """Create hyperopt trials from the given examples."""
    trials = []
    NA = object()
    for tid, ex in enumerate(examples):
        match {"gain": gain, **_} in ex:
            loss = negate_objective(gain)
        else:
            loss = ex["loss"]
        result = {
            "status": STATUS_OK,
            "loss": loss,
        }
        vals = {}
        idxs = {}
        for k, v in zip(
            sorted(params),
            make_features(ex["values"], params, default_placeholder=NA)
        ):
            vals[k] = [v] if v is not NA else []
            idxs[k] = [tid] if v is not NA else []
        misc = {
            "tid": tid,
            "idxs": idxs,
            "vals": vals,
            "cmd": None,
        }
        trials.append({
            "tid": tid,
            "result": result,
            "misc": misc,
            "spec": spec_from_misc(misc),
            "state": JOB_STATE_DONE,
            "owner": None,
            "book_time": None,
            "refresh_time": None,
            "exp_key": None,
        })
    return trials

# Backend:

class HyperoptBackend:
    """The hyperopt backend uses hyperopt for black box optimization."""
    random_backend = RandomBackend()
    current_values = None

    def __init__(self, examples, params, algo=tpe.suggest, rstate=np.random.RandomState(), **kwargs):
        if not examples:
            self.current_values = {}
            return
        space = {
            name: create_space(name, **param_processor.filter_kwargs(param_kwargs))
            for name, param_kwargs in sorted_items(params)
        } |> as_apply
        domain = Domain(self.set_current_values, space)
        trials = Trials()
        trials.insert_trial_docs(examples_to_trials(examples, params))
        FMinIter(
            algo,
            domain,
            trials,
            rstate,
            **kwargs,
        ) |> next
        assert self.current_values is not None
        assert set(self.current_values.keys()) == set(params)

    def set_current_values(self, values):
        assert isinstance(values, dict)
        self.current_values = values
        return {
            "status": STATUS_RUNNING,
        }

    @param_processor.implements_params$(
        backend_name="hyperopt",
        implemented_params=(
        # should match create_space above
        "choice",
        "randrange",
        "uniform",
        "normalvariate",
    ))
    def param(self, name, **kwargs):
        return (name, kwargs) |*> serve_values$(
            serving_values=self.current_values,
            fallback_func=self.random_backend.param,
        )
