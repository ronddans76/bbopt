# Imports:

import sys
import csv
import os.path
from pprint import pprint

import numpy as np

from keras.models import Sequential
from keras.layers import Dense, Activation
from keras.optimizers import SGD
from keras.utils import to_categorical
from keras.regularizers import l1_l2

# Data processing:

data_folder = os.path.join(os.path.dirname(__file__), "data")
house_votes = os.path.join(data_folder, "house_votes.csv") |> np.loadtxt$(dtype=str, delimiter=",")

X = house_votes[:,1:] |> np.vectorize(->
    1 if _ == "y"
    else -1 if _ == "n"
    else 0 if _ == "?"
    else (def -> raise TypeError("unknown vote %r" % _))(_)
)
y = house_votes[:,0] |> np.vectorize(->
    1 if _ == "democrat"
    else 0 if _ == "republican"
    else (def -> raise TypeError("unknown party %r" % _))(_)
) |> to_categorical

train_split = .6*len(X) |> int
validate_split = train_split + .2*len(X) |> int

X_train, X_validate, X_test = X[:train_split], X[train_split:validate_split], X[validate_split:]
y_train, y_validate, y_test = y[:train_split], y[train_split:validate_split], y[validate_split:]

# BBopt setup:
from bbopt import BlackBoxOptimizer
bb = BlackBoxOptimizer(file=__file__)

# Load number of trials:
if len(sys.argv) > 1:
    N = int(sys.argv[1])
else:
    N = 1

# Main loop:
for i in range(N):
    bb.run(backend="scikit-optimize")

    print("\n= %d = (example #%d)" % (i+1, len(bb.get_data()["examples"])+1))

    model = Sequential([
        Dense(
            units=bb.randint("hidden neurons", 1, 15, guess=2),
            input_dim=len(X_train[0]),
            kernel_regularizer=l1_l2(
                l1=bb.uniform("l1", 0, 0.1, guess=0.005),
                l2=bb.uniform("l2", 0, 0.1, guess=0.05),
            ),

        ),
        Activation("relu"),
        Dense(units=2),
        Activation("softmax"),
    ])

    model.compile(
        loss="categorical_crossentropy",
        optimizer=SGD(
            lr=bb.uniform("learning rate", 0, 0.5, guess=0.15),
            decay=bb.uniform("decay", 0, 0.01, guess=0.0005),
            momentum=bb.uniform("momentum", 0, 1, guess=0.5),
            nesterov=bb.getrandbits("nesterov", 1, guess=1) |> bool,
        ),
        metrics=["accuracy"],
    )

    train_history = model.fit(
        X_train,
        y_train,
        epochs=50,
        batch_size=bb.randint("batch size", 1, 32, guess=16),
        verbose=0,
    )

    train_loss, train_acc = train_history.history["loss"][-1], train_history.history["acc"][-1]

    validation_loss, validation_acc = model.evaluate(X_validate, y_validate, verbose=0)

    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)

    bb.remember({
        "training loss": train_loss,
        "training accuracy": train_acc,
        "validation loss": validation_loss,
        "validation accuracy": validation_acc,
        "test loss": test_loss,
        "test accuracy": test_acc,
    })

    bb.minimize(validation_loss)

    pprint(bb.get_current_run())

# Summary of best run:
best_example = bb.get_optimal_run()
print("\n= BEST = (example #%d)" % bb.get_data()["examples"].index(best_example))
pprint(best_example)
